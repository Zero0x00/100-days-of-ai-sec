# Day 23 Adversarial Examples

## When ML Sees Whatâ€™s Not There

Imagine adding a few pixels of noise to a stop signâ€¦  
and suddenly a self-driving car thinks itâ€™s a speed limit sign.  
Thatâ€™s not science fiction â€” thatâ€™s Adversarial Machine Learning in action.

<div><figure><img src="images/day23-1-poster.jpg" alt=""><figcaption></figcaption></figure> <figure><img src="images/day23-2-poster.jpg" alt=""><figcaption></figcaption></figure> <figure><img src="images/day23-3-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day23-4-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day23-5-poster.png" alt=""><figcaption></figcaption></figure><figure><img src="images/day23-6-poster.png" alt=""><figcaption></figcaption></figure></div>

---

## ğŸ¯ What Are Adversarial Examples?

Inputs that are intentionally and subtly modified to fool machine learning models â€” without changing what a human would perceive.

**Examples:**

- ğŸ¼ + invisible noise â¡ï¸ ğŸ¦  
  *(ImageNet misclassification from Goodfellow et al., 2014)*
- ğŸ“ Reworded spam that passes email filters
- ğŸ“¸ Camouflaged clothing that bypasses surveillance AI

These attacks exploit linear weaknesses in high-dimensional models â€” essentially hacking the math behind ML.

---

## ğŸ” Motive of the Attacker

Why craft adversarial examples?

- ğŸ¯ **Evade detection**: Slip past spam filters, malware classifiers, or surveillance tools.
- ğŸ¯ **Trigger misclassification**: Mislead self-driving cars or biometric systems into dangerous decisions.
- ğŸ¯ **Model probing**: Map model boundaries or reverse-engineer behavior.
- ğŸ¯ **Strategic or financial gain**: Disrupt AI-driven systems (ads, pricing, fraud detection) for profit or sabotage.

---

## ğŸ” Security Lens

### âš ï¸ Evasion Attacks

Craft inputs at inference time to bypass detection.  
â†’ *e.g., tweak malware binaries or phishing images*

### âš ï¸ Black-box Attacks

Donâ€™t need access to model internals â€” thanks to transferability, attacks created for one model often work on others.

### âš ï¸ Physical-World Attacks

Stickers on road signs or custom glasses that fool facial recognition.  
Adversarial ML escapes the lab â€” and enters reality.

---

## ğŸ§ª Real-World Examples

- ğŸ”§ Tesla Autopilot fooled by altered road signs â€“ [Tencent Keen Lab]
- ğŸ¢ Google Vision API labeled turtle as rifle
- ğŸª Apple FaceID bypassed by 3D-printed mask (2017 demo)

These are not theoretical flaws â€” theyâ€™ve already been exploited.

---

## ğŸ›¡ Defenses (Imperfect, But Useful)

- âœ… **Adversarial Training** â€“ Train with adversarial examples  
- âœ… **Input Sanitization** â€“ Remove or normalize noise  
- âœ… **Certified Defenses** â€“ e.g., randomized smoothing  
- âœ… **Gradient Masking** â€“ Obfuscate gradients (*but fragile!*)  
- âœ… **Reject Suspicious Inputs** â€“ Flag inputs close to decision boundaries  
- âœ… **Defensive Distillation**

---

## ğŸ“š Key References

- Goodfellow et al. (2014) â€“ *â€œExplaining and Harnessing Adversarial Examplesâ€*
- Kurakin et al. (2016) â€“ *â€œAdversarial Examples in the Physical Worldâ€*
- Athalye et al. (2018) â€“ *â€œObfuscated Gradients Give a False Sense of Securityâ€*

ğŸ”— [Adversarial.js by Kenny Song](https://kennysong.github.io/adversarial.js/)

---

## ğŸ’¬ Question for You

**How do you test your models for adversarial robustness?**  
Should it be part of every AI model's CI/CD pipeline?

---

## ğŸ“… Coming Up

**Day 23 â†’ Data Poisoning Attacks** â€“ when the training data becomes the attack vector. ğŸ”¥

---

## ğŸ”— Catch Up on Day 22

- [LinkedIn Post](https://www.linkedin.com/posts/mohd--arif_100daysofaisec-100daysofaisec-aisecurity-activity-7328488388116774915-AdyP)
