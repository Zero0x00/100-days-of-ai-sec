---
description: Loss Functions â€” The Modelâ€™s Moral Compass
---

# Day 7 Loss Functions

***

![Day 07 Poster](images/day07-poster.png)

Today I explored **Loss Functions** â€” the core ingredient that tells a machine learning model how â€œwrongâ€ it is, and how to improve.

***

## ğŸ”¹ Quick Primer

* A **loss function** measures the gap between predicted and true values
* The goal of training is to **minimize this loss**
* Models update their internal weights based on the loss to get closer to the truth

ğŸ§  Think of a loss function as the **moral compass** of a model â€” but a poorly chosen or manipulated one can send it in the wrong direction.

***

## ğŸ” Security Lens: How Loss Functions Can Be Attacked

### âš ï¸ Loss Hijacking

Attackers subtly poison the training data so the **loss function rewards the wrong behavior**.

ğŸ¦´ _Analogy:_ You're training a dog to sit, giving treats. But someone gives treats when it jumps instead.\
Now the dog thinks **jumping is â€œcorrectâ€** â€” same with your model.

***

### âš ï¸ Gradient Masking

Some defences modify loss functions to hide gradients â€” but attackers often find ways around them.

ğŸ§­ _Analogy:_ Youâ€™re playing â€œhot and coldâ€ to find treasure.\
Normally, â€œhotâ€ means youâ€™re close. But now someone fakes the sound.\
You keep hearing â€œhotâ€ even when far â€” **youâ€™re misled into thinking you're doing well**.

***

### âš ï¸ Poor Loss Choices

Choosing the **wrong loss function** makes the model optimize the **wrong thing**.

ğŸŒ¶ï¸ _Analogy:_ You judge recipes only by how spicy they are, ignoring taste or balance.\
Youâ€™ll end up with a dish that's **all spice, no flavor** â€” a model good at the wrong goal.

***

## ğŸ“š Key References

* Goodfellow et al. (2014): _Explaining and Harnessing Adversarial Examples_
* Carlini & Wagner (2017): _Towards Evaluating the Robustness of Neural Networks_
* [IBM Tech Blog: Loss Function Risks](https://lnkd.in/gk8pq2Hp)

***

## ğŸ’¬ Question for You

**Whatâ€™s the strangest model failure** youâ€™ve seen because of a poorly chosen loss function?\
Letâ€™s discuss ğŸ‘‡

***

ğŸ“… **Up Next**: We dive into **Gradient Descent** â€” how models â€œwalk downhillâ€ to find better answers ğŸ”½

ğŸ”— **Missed Day 6?** [Catch up here](https://lnkd.in/gsMYGXwr)

***

**#100DaysOfAISec â€“ Day 7 Post**\
\#AISecurity #MLSecurity #MachineLearningSecurity #LossFunctions #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #100DaysChallenge #ArifLearnsAI #LinkedInTech
