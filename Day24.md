# ğŸ§µ Day 24 of Data Poisoning Attacks

## When Your Model Learns to Betray You

> What if an attacker tweaks just **a few training samples** â€” and your model suddenly starts **making wrong decisions**, **leaking data**, or even **obeying secret triggers**?

Welcome to **Data Poisoning** â€” where malicious data trains malicious models.

<div><figure><img src="images/day24-1-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day24-2-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day24-3-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day24-4-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src="images/day24-5-poster.png" alt=""><figcaption></figcaption></figure><figure><img src="images/day24-6-poster.png" alt=""><figcaption></figcaption></figure></div>

---

## ğŸ§  What Is Data Poisoning?

Itâ€™s when an attacker **injects manipulated samples into your training data** to:

- ğŸŸ¥ **Break** the model *(Availability attack)*
- ğŸ¯ **Subvert** specific behavior *(Targeted attack)*
- ğŸ•µï¸â€â™‚ï¸ **Backdoor** the model silently *(Clean-label attack)*
- ğŸ§© **Leak** private data during inference *(Privacy attack)*

> âš ï¸ Most poisoned samples are **subtle and statistically valid**, so they bypass basic data checks.

---

## ğŸ¯ Why Would an Attacker Poison Your Model?

Different motives, same danger:

- ğŸ”¨ **Sabotage** a systemâ€™s accuracy or availability  
- ğŸ§¬ **Create secret triggers** only the attacker knows  
- ğŸ”“ **Bypass security filters** like spam or malware detection  
- ğŸ’£ **Insert logic bombs** triggered in production  
- ğŸ•µï¸â€â™€ï¸ **Extract private information** from training data  
- ğŸ§  **Manipulate AI behavior** in social, political, or economic contexts  

---

## ğŸ”¬ Attack Types Compared

| Attack Type                   | Goal                                   | Impact Scope | Stealth Level | Example                                                 |
|------------------------------|----------------------------------------|--------------|----------------|---------------------------------------------------------|
| ğŸŸ¥ Availability Attack        | Degrade model performance *for everyone* | Global       | Medium         | Poisoning spam filter with mislabeled ham               |
| ğŸ¯ Targeted Misclassification | Fool model *only on specific inputs*   | Localized    | High           | Misclassify face when attacker wears special glasses    |
| ğŸ§ª Clean-label Poisoning      | Train on *legit-looking poisoned samples* | Subtle & Persistent | Very High | One cat image causes test-time face recognition error   |

---

## ğŸ§ª Real-World Examples

- **Microsoft Tay** â€” poisoned by malicious tweets â†’ started making offensive remarks  
- **Google Perspective** â€” adversarial users injected toxic-but-acceptable phrases  
- **LLM Alignment datasets** â€” found to contain biased/misleading training prompts  

---

## ğŸ›¡ï¸ Defenses â€” General + Specialized

### âœ… General Defense Principles

- Use **robust training** (e.g., differential privacy, trimmed loss)  
- Audit your data pipeline â€” especially crowdsourced/third-party  
- Monitor **data provenance and contributor reputation**  
- Apply **outlier detection**, deduplication, and label smoothing  

---

### ğŸ”¬ Specialized Defenses by Attack Type

| Attack Type                   | Specialized Defenses                                                                 |
|------------------------------|---------------------------------------------------------------------------------------|
| ğŸŸ¥ Availability Attack        | - Trimmed loss functions (e.g., generalized cross-entropy) <br> - Influence functionâ€“based sanitization |
| ğŸ¯ Targeted Misclassification | - Activation clustering <br> - Neural Cleanse for trigger reverse-engineering         |
| ğŸ§ª Clean-label Poisoning      | - Spectral Signature analysis <br> - Detect high-influence samples (e.g., Shapley scores) |

> âš ï¸ **No silver bullet exists yet** â€” most of these are **active research areas**.

---

## ğŸ“š Key References

- Steinhardt et al. (2017) â€” *Certified Defenses for Data Poisoning*  
- Shafahi et al. (2018) â€” *Poison Frogs: Targeted Clean-label Poisoning*  
- Jagielski et al. (2018) â€” *Manipulating Machine Learning via Poisoning*  

---

## ğŸ’¬ Reflection Questions

- How much **trust** do you place in your training data sources?  
- Do you **audit and sanitize** your datasets before each retraining cycle?

---

## ğŸ“… Up Next

**Day 25 â€” Model Backdooring**:  
> When your model hides a secret â€œtrigger wordâ€ that only the attacker knows. ğŸ˜ˆğŸ§ 
