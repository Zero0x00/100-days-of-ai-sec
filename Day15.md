# Day 15 Activation Functions

***

![Day 15 Poster](images/day15-poster.png)

## Activation Functions: Tiny Math Gates That Can Break Your Model

Think activation functions (ReLU, Sigmoid, Tanh, Softmax) are just boring math between layers? Think again!\
They shape how your model learns **and** how it can be attacked.

***

### ğŸ” Why They Matter:

* Introduce non-linearity, enabling models to learn complex patterns
* Decide if a neuron "fires"
* Without them, deep learning is just glorified linear regression!

***

### ğŸ“Œ Common Types

* **ReLU**: `max(0, x)` â€” Fast, promotes sparsity, but neurons can â€œdieâ€ (stay inactive)
* **Sigmoid**: S-shaped, squashes outputs to (0,1) â€” great for binary tasks, but prone to vanishing gradients
* **Tanh**: Like Sigmoid, but outputs (-1,1) â€” centers data for better gradients
* **Softmax**: Turns raw scores into probabilities â€” ideal for classification

***

### ğŸ” Security Lens:

#### ğŸ“‰ Vanishing Gradients (Sigmoid, Tanh)

* **Problem**: Gradients shrink in deep networks, stalling learning
* ğŸ’¥ _Attack_: Poisoned data causes â€œgradient obfuscation,â€ weakening deep layers
* ğŸ›¡ï¸ _Mitigation_: Use ReLU/variants, Layer Normalization

#### ğŸ’€ Dead ReLU Exploits

* **Problem**: ReLU outputs 0 for negative inputs â€” neurons can get stuck "off"
* ğŸ’¥ _Attack_: Crafted inputs force critical neurons into this "dead" state
* ğŸ›¡ï¸ _Mitigation_: Use Leaky ReLU or Parametric ReLU (PReLU)

#### ğŸ¤¯ Softmax Confidence Overload

* **Problem**: Softmax assigns high confidence to even nonsensical inputs
* ğŸ’¥ _Attack_: Fooling models (e.g., near-blank image â†’ â€œStop Signâ€ with 99% confidence) â€” huge risk in autonomous systems!
* ğŸ›¡ï¸ _Mitigation_: Output regularization, Confidence Calibration (e.g., Temperature Scaling)

#### ğŸ‘» Gradient Exploitation (Smooth Activations like Sigmoid/Tanh)

* **Problem**: Smooth gradients are easier for attackers to reverse engineer
* ğŸ’¥ _Attack_: Subtle adversarial examples that fool the model with high confidence
* ğŸ›¡ï¸ _Mitigation_: Add gradient noise, use adversarial training

***

### ğŸ“š Key References

* Goodfellow et al. (2014): Introduced adversarial examples
* Carlini & Wagner (2017): Robustness testing techniques
* Su et al. (2019): One-pixel attacks on deep neural networks

***

### ğŸ’¬ Letâ€™s Discuss!

* ML engineers: How do you prevent dead ReLUs in production?
* Security pros: Seen activation-related attacks in the wild?
* Everyone: How do you balance model performance vs. robustness?

***

ğŸ“… **Tomorrow**: Deep vs. Shallow Models â€” Which fails more gracefully under attack?

ğŸ”— Missed Day 14? [https://lnkd.in/gvqKwDEK](https://lnkd.in/gvqKwDEK)

***

\#100DaysOfAISec #AISecurity #MLSecurity #MachineLearningSecurity #ActivationFunctions #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #100DaysChallenge #ArifLearnsAI #LinkedInTech
