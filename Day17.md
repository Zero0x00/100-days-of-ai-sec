# Day 17 Hyperparameter Tuning

***

![Day 17 Poster](images/day17-poster.png)

Today I explored one of the most crucial steps in ML development: **Hyperparameter Tuning** â€” where we squeeze out performance from a model by tweaking knobs like learning rate, depth, dropout, etc.

But tuning isnâ€™t just about better accuracy â€” done carelessly, it opens unseen vulnerabilities.

***

### ğŸ”§ What is Hyperparameter Tuning?

Parameters set before training (not learned):

* Learning rate
* Max depth (trees)
* Number of layers/neurons
* Batch size, dropout, regularization, etc.

**Techniques:**

* Grid search ğŸ”²
* Random search ğŸ²
* Bayesian optimization ğŸ“ˆ
* AutoML ğŸ”

***

### ğŸ” Security Lens â€” Where Tuning Turns Risky

#### **Overfitting via Aggressive Tuning**

Highly tuned models can memorize noise or poisoned data.

ğŸ’¥ _Example_: In financial fraud detection, hyperparameter tuning over poisoned data results in consistent fraud cases being ignored.\
ğŸ›¡ï¸ _Mitigation_: Validate on clean, held-out test sets; monitor for data poisoning.

***

#### **Under-Regularized Models**

Improper dropout/L2 values â†’ models generalize poorly and are vulnerable to adversarial inputs.

ğŸ’¥ _Example_: A CNN for document classification gets tuned for perfect accuracy but becomes sensitive to minor word order changes crafted by an attacker.\
ğŸ›¡ï¸ _Mitigation_: Apply minimum regularization; test against adversarial examples.

***

#### **Privacy Leakage via â€œBest Fitâ€**

Tuning may amplify memorization, causing models to regurgitate sensitive data points.

ğŸ’¥ _Example_: A tuned model on medical data leaks a real patientâ€™s diagnosis when queried.\
ğŸ›¡ï¸ _Mitigation_: Use differential privacy techniques; audit for data leakage.

***

#### **Adversarial Tuning Attacks (emerging)**

In shared ML pipelines (e.g., cloud training), attackers can manipulate tuning inputs to sabotage the final model.\
ğŸ›¡ï¸ _Mitigation_: Validate tuning inputs; monitor for anomalies in shared environments.

***

### ğŸ“š Key References

* _Carlini et al. (2021)_: _Extracting Training Data from Language Models_
* _Kandemir et al. (2022)_: _Hyperparameter Stealing Attacks on ML APIs_
* [Hands-On Hyperparameter Tuning with Scikit-Learn: Tips and Tricks](https://lnkd.in/g-PDVPuU)

***

### ğŸ’¬ Question for You:

**Do you automate hyperparameter tuning, and if so, how do you monitor for security regressions during the process?**

***

ğŸ“… **Tomorrow**: We explore _Bias & Variance_ âš–ï¸

ğŸ”— **Missed Day 16?**\
[https://lnkd.in/gVMyWMSJ](https://lnkd.in/gVMyWMSJ)

***

**#100DaysOfAISec** #AISecurity #MLSecurity #MachineLearningSecurity #ModelTunning #Hyperparameter #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #100DaysChallenge #ArifLearnsAI #LinkedInTech
