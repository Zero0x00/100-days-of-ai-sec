# Day 21 Model Inversion

## Model Inversion Attacks â€“ Reconstructing Faces from ML Models ğŸ¤¯

<div><figure><img src=".gitbook/assets/day21-4-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src=".gitbook/assets/day21-5-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src=".gitbook/assets/day21-3-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src=".gitbook/assets/day21-1-poster.png" alt=""><figcaption></figcaption></figure> <figure><img src=".gitbook/assets/day21-2-poster.png" alt=""><figcaption></figcaption></figure></div>

> Imagine asking an AI model about diabetes risk â€” and reconstructing a patientâ€™s face.\
> Thatâ€™s not sci-fi. Thatâ€™s model inversion â€” and itâ€™s happening now.

***

### ğŸ” What Is Model Inversion?

Model Inversion Attacks aim to **reconstruct sensitive input features** (like faces, DNA, or text) using a modelâ€™s **outputs** â€” especially when itâ€™s overfit or exposes **confidence scores**.

ğŸ§  When trained on PII-rich data, models can unintentionally **leak individual details** from the training set.

***

### ğŸ§ª Real-World Example

**ğŸ“š Fredrikson et al. (2015):**

* Trained a model to predict warfarin dosage.
* By probing the model with known inputs, they **reconstructed genetic markers** of real patients.

**ğŸ­ Facial Recognition:**\
Similar attacks recreated **faces** from facial recognition models â€” just by analyzing output scores.

***

### ğŸ§  Different Models, Different Risks

* ğŸ–¼ï¸ **Image Models**: Attackers can recreate training images.
* ğŸ“„ **Text Models (LLMs)**: May regenerate secrets, passwords, or emails.
* ğŸ“Š **Graph Models**: Can leak node attributes or private edges.

***

### ğŸ” Defenses (Summary)

âœ… Donâ€™t expose **confidence scores** or **logits** publicly.\
âœ… Train with **differential privacy**.\
âœ… Use **dropout/regularization** to reduce memorization.\
âœ… **Monitor** for unusual query patterns (e.g., mass probing).

***

### â— Model Inversion vs Membership Inference

| Attack Type              | Goal                           | Input             | Output               |
| ------------------------ | ------------------------------ | ----------------- | -------------------- |
| **Model Inversion**      | Rebuild sensitive data         | Model outputs     | Reconstructed inputs |
| **Membership Inference** | Detect if data was in training | Model + datapoint | Yes / No             |

***

### ğŸ’¬ Thought Starter

> Would you expose **top-5 predictions with confidence scores** in production?\
> How do you balance **model utility** vs **user privacy**?

***

### ğŸ”— Resources

* ğŸ“™ **Recap of Day 1â€“20**: [LinkedIn Post](https://www.linkedin.com/posts/mohd--arif_100daysofaisec-100daysofaisec-mlsecurity-activity-7327558186935156736-619f)
* ğŸ“˜ **GitBook (All Posts)**: [100 Days of AI Sec](https://arif-playbook.gitbook.io/100-days-of-ai-sec)

***

### ğŸ·ï¸ Tags

\#100DaysOfAISec #AISecurity #MLSecurity #ModelInversion #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #MachineLearningSecurity #100DaysChallenge #ArifLearnsAI #LinkedInTech
