---
description: Overfitting vs. Underfitting â€” Why Theyâ€™re Security Nightmares
---

# Day 4 Overfitting vs Underfitting

***

![Day 04 Poster](images/day04-poster.png)

Today I explored two core ML concepts that every AI system struggles with:

***

## ğŸ”¹ Overfitting

When a model performs great on **training data** but poorly on **real-world data** (it â€œmemorizesâ€ instead of generalizing).\
**Example**: A kid sees only black dogs during training and learns "All dogs are black."\
Now when shown a brown dog, he says, "Thatâ€™s not a dog."\
He memorized, didnâ€™t generalize.

***

## ğŸ”¹ Underfitting

When a model is **too simple** to capture patterns â€” it performs poorly **even on training data**.\
**Example**: Another kid is told, "Animals with four legs are dogs."\
Now he calls a cat, a lion, even a table â€” a dog.\
He didnâ€™t learn enough details.

These arenâ€™t just training challenges â€” they have **real security consequences** if ignored.

***

## ğŸ” Security Lens

### âš ï¸ Overfitting Risks

* **Membership Inference Attacks**\
  If a model memorizes its training data, attackers can probe it to find out whether a **specific datapoint** was used â€” a major **privacy risk** in sectors like healthcare or finance.\
  **Example**: A model trained on real patient data might react slightly differently if you ask, â€œWas John Doeâ€™s tumor data in your training set?â€\
  This helps an attacker **confirm the presence of sensitive data**.
* **Model Inversion**\
  Overfitted models can **leak internal data characteristics**.\
  Attackers can use output scores to **reconstruct** sensitive input features.\
  **Example**: A facial recognition model trained on private ID photos can be queried repeatedly.\
  The attacker may eventually synthesize a **lookalike face** from training data â€” even without seeing the original image.

> ğŸ§  Itâ€™s like asking enough yes/no questions about a secret photo until you can draw a disturbingly accurate replica.\
> Scary, right?

***

### âš ï¸ Underfitting Risks

* **Missed Edge Cases**\
  Underfit models are **too simple** to capture rare or abnormal patterns â€” a critical issue in **security**, where attackers often behave abnormally.\
  **Example**: An anti-fraud model might miss a **low-volume, high-value scam** if it never learned the subtle patterns behind it.

***

## ğŸ“š References

* _Shokri et al., 2017_ â€“ Membership Inference Attacks Against Machine Learning Models
* [Blog by Kush](https://lnkd.in/d3fjc4pE)
* [ML Overfitting & Privacy - LinkedIn Article](https://lnkd.in/dyRdANmU)

***

## ğŸ’¬ Question

How do you **balance accuracy vs. privacy** in ML models at your company?

***

ğŸ“… **Tomorrow**: Iâ€™ll dig into **Decision Trees and Random Forests** â€” and why **randomness is good** ğŸ²

ğŸ”— **Previous Post**: [Day 3](https://lnkd.in/dF6N_Da9)

***

**Day 4/100 â€” Beginning My #100DaysOfAISec Journey**\
\#AISecurity #MLSecurity #DeepLearning #CyberSecurity #100DaysChallenge #LearningInPublic #ArifLearnsAI
