# Day 13 Naive Bayes

***

![Day 13 Poster](images/day13-poster.png)

Today, I dove into one of the oldest and surprisingly effective ML classifiers â€” **Naive Bayes**.

ğŸ”¹ Itâ€™s based on Bayesâ€™ Theorem:\
`P(Class | Features) = [ P(Features | Class) Ã— P(Class) ] / P(Features)`

ğŸ”¹ The â€œnaiveâ€ part? It assumes all features are independent â€” rarely true in reality, but often good enough, especially for text classification.

> Naive Bayes is like a doctor diagnosing a patient by looking at symptoms one at a time, assuming each symptom (like cough, fever, fatigue) occurs independently.\
> In reality, symptoms often correlate â€” but this simplified model still gets the diagnosis right surprisingly often.

***

### ğŸ› ï¸ Common Use Cases

* âœ… Spam Filtering
* âœ… Text Classification
* âœ… Intrusion Detection Systems (IDS)

ğŸ§  Despite its simplicity, Naive Bayes performs surprisingly well â€” particularly on high-dimensional datasets like emails and documents.

***

### ğŸš§ Limitations

* Struggles with non-linear relationships or complex interactions between features.
* Can be sensitive to skewed class distributions if not properly calibrated.

But that independence assumption? A sweet spot for attackers.

***

### ğŸ” Security Lens

#### âš ï¸ Independence Assumption Abuse

Attackers inject correlated features to game the classifier.\
&#xNAN;_&#x45;xample:_ A spam email might include benign terms like â€œinvoiceâ€ or â€œteam updateâ€ to lower its spam score and evade detection.

#### âš ï¸ Feature Poisoning

Adversaries inject mislabeled or crafted data into the training set to skew feature probabilities, corrupting the model's logic.

#### âš ï¸ Privacy Leaks via Probabilistic Outputs

Naive Bayes outputs probabilities. Confidence scores can leak info about the training data, enabling **membership inference attacks**.

***

### ğŸ“š Key References

* Rubinstein et al. (2009) â€” _Privacy-Preserving Classification_
* Lowd & Meek (2005) â€” _Adversarial Learning in Naive Bayes Spam Filters_
* Biggio et al. (2013) â€” _Evasion Attacks against Machine Learning at Test Time_

***

### ğŸ’¬ Question

**How much do you trust simple models like Naive Bayes in high-stakes systems?**\
Letâ€™s discuss â€” sometimes old tools still hold up, but only when you know their limits.

***

ğŸ“… **Up next (Day 14):** Support Vector Machines (SVM) â€” and how attackers can shift the decision boundary to their advantage âš–ï¸

ğŸ”— Missed Day 12? Catch up here: [https://lnkd.in/ghkbH6Nb](https://lnkd.in/ghkbH6Nb)

***

\#100DaysOfAISec #AISecurity #MLSecurity #MachineLearningSecurity #NaiveBayes #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #100DaysChallenge #ArifLearnsAI #LinkedInTech
