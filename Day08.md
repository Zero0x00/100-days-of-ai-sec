---
description: Loss Functions â€” The Modelâ€™s Moral Compass
---

# Day 8 Gradient Descent

***

![Day 08 Poster](images/day08-poster.png)

What if I told you almost every AI model is just trying to **roll down a hill without slipping**? âš¡

Welcome to **Gradient Descent** â€” the powerful algorithm behind how machines learn.

***

## ðŸ”¹ What is Gradient Descent?

Gradient Descent helps models find the lowest point on a loss curve â€” i.e., **minimum error** â€” by taking small, calculated steps downhill.

ðŸ§­ Itâ€™s like hiking down a foggy, bumpy trail:

* The slope tells you which direction to step
* Each step adjusts the modelâ€™s weights
* The goal? Reach the valley of **minimum loss**

***

## ðŸ” Security Lens: How Attackers Hijack the Learning Path

Gradient Descent may be smart â€” but attackers know how to poison the path. Hereâ€™s how:

### âš ï¸ Misleading Road Signs (Gradient Manipulation)

ðŸ“Œ _Data poisoning_ makes the gradient point in the wrong direction â€” training the model to learn **wrong behaviors**.

> Think of fake road signs that send you off-course while hiking.

***

### âš ï¸ Banana Peels on the Trail (Adversarial Examples)

ðŸ“Œ At test time, attackers add tiny changes to inputs that **donâ€™t look wrong** â€” but cause major misclassifications.

> Like stepping on an invisible banana peel â€” you slip, even though the path looks fine.

***

### âš ï¸ Fake Shortcuts and Hidden Traps (Optimization Traps)

ðŸ“Œ Attackers craft **malicious loss landscapes** to trap the model in **bad local minima**.

> You're tricked into thinking youâ€™ve reached the valley â€” but itâ€™s just a ditch.

***

## ðŸ“š Key References

* Biggio et al. (2012): _Poisoning Attacks Against Support Vector Machines_
* Goodfellow et al. (2015): _Explaining and Harnessing Adversarial Examples_
* Jovian: [Gradient Descent Deep Dive](https://lnkd.in/g28hPX_3)
* Aditya Bharathi: [Security Perspectives](https://lnkd.in/gNV_pfRT)

***

## ðŸ’¬ Question for You

**Have you battled frustrating optimization issues while training your models?**\
Iâ€™d love to hear your story ðŸ‘‡

***

ðŸ“… **Up Next**: How Neural Networks become brainy "universal approximators" â€” and why thatâ€™s a double-edged sword ðŸ§ âš¡

ðŸ”— **Missed Day 7?** [Catch it here](https://lnkd.in/guKzZUMC)

***

**#100DaysOfAISec â€“ Day 8 Post**\
\#AISecurity #MLSecurity #MachineLearningSecurity #GradientDescent #CyberSecurity #AIPrivacy #AdversarialML #LearningInPublic #100DaysChallenge #ArifLearnsAI #LinkedInTech
