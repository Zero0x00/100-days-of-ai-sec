---
description: Types of Machine Learning â€” and Their Security Implications
---

# Day 2 Type of ML



***

![Day 02 Poster](images/day02-poster.png)

Today, I explored the **three main types of Machine Learning**:

***

## ğŸ”¹ Supervised Learning

The model learns from **labelled data** (e.g., spam vs. not spam).\
Think of it as giving the machine both the **question and the correct answer**. The model learns the pattern and applies it to new, unseen data.\
**Example**: A spam filter trained on emails labelled as "spam" or "not spam" learns how to classify future emails.

***

## ğŸ”¹ Unsupervised Learning

Finds **hidden patterns in unlabeled data** (e.g., customer segmentation).\
You provide raw data with no labels, and the model figures out the structure on its own â€” discovering both the questions and the answers.\
**Example**: A company uploads thousands of customer profiles. The model clusters them into groups like "frequent buyers" or "weekend shoppers" â€” even though these labels were never explicitly given.

***

## ğŸ”¹ Reinforcement Learning

Agents learn through **trial and error in an environment** (e.g., self-driving cars, game AIs).\
**Example**: Imagine a teacher saying, â€œThis answer isnâ€™t good enough for a 10/10.â€ You revise, submit again, and repeat until the teacher says, â€œPerfect!â€\
Thatâ€™s how agents improve â€” by **maximizing rewards through feedback loops**.

***

## ğŸ” Security Lens

Each learning type introduces unique **attack surfaces and risks**:

### âš ï¸ Supervised Learning

* **Risk**: Data poisoning â€” attackers manipulate training data or flip labels to mislead the model.
* **Example**: Injecting malicious, but labelled as â€œsafe,â€ transactions into a fraud detection system to teach it that fraud is normal.

### âš ï¸ Unsupervised Learning

* **Risk**: Cluster poisoning â€” attackers insert noisy or misleading data to corrupt clusters.
* **Example**: In a system clustering user behaviour to detect anomalies (e.g., insider threats), an attacker floods it with fake patterns. This causes legitimate users to be misclassified, hiding real threats in the noise.

### âš ï¸ Reinforcement Learning

* **Risk**: Reward hacking â€” agents find unintended shortcuts that game the system.
* **Example**: An RL agent designed to secure a system might learn to **turn off logging or monitoring features** to "reduce threats" â€” achieving high rewards without actually improving security.

> ğŸ“ **Note**: Different attack techniques exist depending on the **lifecycle phase** of the model (training, deployment, inference, etc).

***

## ğŸ“ Resources

* [ML Crash Course (Google)](https://lnkd.in/gsUcZgVF)
* [Reinforcement Learning Overview (DeepMind)](https://lnkd.in/gPXTEH7d)

***

## ğŸ“„ Papers I Plan to Read

* _Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning_
* _Intriguing Properties of Neural Networks_

***

Tomorrow: Iâ€™ll dive into **Regression vs. Classification** â€” and how each can be abused.\
Have you seen AI being misused in your domain? Letâ€™s connect.

ğŸ“¢ **Follow along**, share your learnings, and drop ideas below!

ğŸ”— **Previous Post**: [Day 1](https://lnkd.in/gxHkzd4x)

***

**Day 2/100 â€” #100DaysOfAISec Journey**\
\#AISecurity #MLSecurity #DeepLearning #CyberSecurity #100DaysChallenge #LearningInPublic #ArifLearnsAI
